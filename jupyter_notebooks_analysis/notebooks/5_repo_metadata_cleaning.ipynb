{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5d77b91859f19"
   },
   "source": [
    "# Repo Metadata and Readme Cleaning\n",
    "\n",
    "This notebook cleans the repo metadata and readme files downloaded for each repository containing at least one Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "b431e3dbb50ce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "b54dbd7b86e8a"
   },
   "source": [
    "## Check for Completeness\n",
    "\n",
    "Now we should check that we have downloaded data for each repo, the readme for each repo, and that this data is complete.The first thing will be to see if we have a metadata file and a readme file for every repository in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "comet_cell_id": "5338530b7b546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193616, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_nb</th>\n",
       "      <th>repo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10950</td>\n",
       "      <td>13769471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4483</td>\n",
       "      <td>79205031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4004</td>\n",
       "      <td>91523680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3636</td>\n",
       "      <td>68774322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3074</td>\n",
       "      <td>55120679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_nb   repo_id\n",
       "0   10950  13769471\n",
       "1    4483  79205031\n",
       "2    4004  91523680\n",
       "3    3636  68774322\n",
       "4    3074  55120679"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull in notebook metadata\n",
    "df_nbs = pd.read_csv('../data/csv/nb_metadata.csv')\n",
    "df_nbs.rename(columns = {'Unnamed: 0':'nb_id'}, inplace = True)\n",
    "\n",
    "# create df of just the repos for each of our notebooks\n",
    "repo_counts = df_nbs['repo_id'].value_counts()\n",
    "df_repos = repo_counts.to_frame(\"num_nb\")\n",
    "df_repos['repo_id'] = df_repos.index \n",
    "df_repos.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_repos.shape)\n",
    "df_repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "4fae23838b"
   },
   "source": [
    "Let's turn this into a list of ids that we can quickly compare with the list of ids from both the metadata we actually downloaded and the readme files we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "comet_cell_id": "ef4f57b4bf8af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2058,    66233,    87831, ..., 97245373, 97246456, 97255702])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_repos = df_repos['repo_id'].as_matrix()\n",
    "np.sort(expected_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "comet_cell_id": "bc1193da57a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 193616 repo metadata files\n",
      "We have 193616 repo readme files\n"
     ]
    }
   ],
   "source": [
    "repo_metadata_ids = [int(f.split('.')[0].split('_')[1]) for f in os.listdir('../data/api_results/repo_metadata') if f[-5:] == '.json']\n",
    "repo_readme_ids = [int(f.split('.')[0].split('_')[1]) for f in os.listdir('../data/readmes') if f[-5:] == '.json']\n",
    "\n",
    "repo_metadata_ids.sort()\n",
    "repo_readme_ids.sort()\n",
    "\n",
    "print('We have %s repo metadata files' % len(repo_metadata_ids))\n",
    "print('We have %s repo readme files' % len(repo_metadata_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "836fa78826687"
   },
   "source": [
    "So it looks like we have the same number of files as repos in our list. Now we should compare these lists to make sure the numbers are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "542a6a0f392f"
   },
   "outputs": [],
   "source": [
    "repo_metadata_ids = np.array(repo_metadata_ids)\n",
    "repo_readme_ids = np.array(repo_readme_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "comet_cell_id": "3bc7590aae851"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_repo_metadata = np.setdiff1d(repo_metadata_ids,expected_repos, assume_unique=True)\n",
    "len(missing_repo_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "comet_cell_id": "f5dd088763d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_repo_readmes = np.setdiff1d(expected_repos, repo_readme_ids,  assume_unique=True)\n",
    "len(missing_repo_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "9548558070762"
   },
   "source": [
    "So our first check is done. We have a metadata file and a readme file for each repo. Now to check that these have content in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5e743672bd3f9"
   },
   "source": [
    "## Metadata Content Completeness\n",
    "Let's see what content we can inspect from the repo metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "comet_cell_id": "407d6f59b9753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'name', 'full_name', 'owner', 'private', 'html_url', 'description', 'fork', 'url', 'forks_url', 'keys_url', 'collaborators_url', 'teams_url', 'hooks_url', 'issue_events_url', 'events_url', 'assignees_url', 'branches_url', 'tags_url', 'blobs_url', 'git_tags_url', 'git_refs_url', 'trees_url', 'statuses_url', 'languages_url', 'stargazers_url', 'contributors_url', 'subscribers_url', 'subscription_url', 'commits_url', 'git_commits_url', 'comments_url', 'issue_comment_url', 'contents_url', 'compare_url', 'merges_url', 'archive_url', 'downloads_url', 'issues_url', 'pulls_url', 'milestones_url', 'notifications_url', 'labels_url', 'releases_url', 'deployments_url', 'created_at', 'updated_at', 'pushed_at', 'git_url', 'ssh_url', 'clone_url', 'svn_url', 'homepage', 'size', 'stargazers_count', 'watchers_count', 'language', 'has_issues', 'has_projects', 'has_downloads', 'has_wiki', 'has_pages', 'forks_count', 'mirror_url', 'open_issues_count', 'forks', 'open_issues', 'watchers', 'default_branch', 'permissions', 'network_count', 'subscribers_count'])\n",
      "\n",
      "dict_keys(['login', 'id', 'avatar_url', 'gravatar_id', 'url', 'html_url', 'followers_url', 'following_url', 'gists_url', 'starred_url', 'subscriptions_url', 'organizations_url', 'repos_url', 'events_url', 'received_events_url', 'type', 'site_admin'])\n"
     ]
    }
   ],
   "source": [
    "with open('../data/api_results/repo_metadata/repo_%s.json' % repo_metadata_ids[0]) as f:\n",
    "    test_file = json.load(f)\n",
    "    print(test_file.keys())\n",
    "    print('')\n",
    "    print(test_file['owner'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5a46a0e40d12c"
   },
   "source": [
    "Based on these values, I think we want:\n",
    "1. id\n",
    "2. owner id\n",
    "3. ownder login\n",
    "4. owner type\n",
    "5. name\n",
    "6. description\n",
    "7. private\n",
    "8. fork\n",
    "9. html_url\n",
    "10. language\n",
    "11. forks_count\n",
    "12. stargazers_count\n",
    "13. watchers_count\n",
    "13. subscribers_count\n",
    "14. network_count\n",
    "14. size\n",
    "15. open_issues_count\n",
    "16. topics\n",
    "17. has_issues\n",
    "18. has_wiki\n",
    "19. has_pages\n",
    "20. has_downloads\n",
    "21. pushed_at\n",
    "22. created_at\n",
    "22. updated_at\n",
    "\n",
    "We can compile this information from the separate .json files into one repo metadata dataframe that we can then save as a CSV file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "comet_cell_id": "4a3d154b22f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 193616 repo data files processed\n",
      "10000 / 193616 repo data files processed\n",
      "20000 / 193616 repo data files processed\n",
      "30000 / 193616 repo data files processed\n",
      "40000 / 193616 repo data files processed\n",
      "50000 / 193616 repo data files processed\n",
      "60000 / 193616 repo data files processed\n",
      "70000 / 193616 repo data files processed\n",
      "80000 / 193616 repo data files processed\n",
      "90000 / 193616 repo data files processed\n",
      "100000 / 193616 repo data files processed\n",
      "110000 / 193616 repo data files processed\n",
      "120000 / 193616 repo data files processed\n",
      "130000 / 193616 repo data files processed\n",
      "140000 / 193616 repo data files processed\n",
      "150000 / 193616 repo data files processed\n",
      "160000 / 193616 repo data files processed\n",
      "170000 / 193616 repo data files processed\n",
      "180000 / 193616 repo data files processed\n",
      "190000 / 193616 repo data files processed\n",
      "\n",
      "We have 193026 notebooks\n"
     ]
    }
   ],
   "source": [
    "all_repos = []\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = '../logs/repo_metadata_cleaning_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "for i, n in enumerate(repo_metadata_ids):\n",
    "    \n",
    "    # keep track of our progress\n",
    "    if i % 10000 == 0:\n",
    "        print(\"%s / %s repo data files processed\" % (i, len(repo_metadata_ids)))\n",
    "    \n",
    "    with open('../data/api_results/repo_metadata/repo_%s.json' % n, 'r') as json_file:\n",
    "\n",
    "        try:\n",
    "            j = json.load(json_file)\n",
    "            \n",
    "            repo_stats = {}\n",
    "            repo_stats['id'] = j['id']\n",
    "            repo_stats['owner_id'] = j['owner']['id']\n",
    "            repo_stats['owner_login'] = j['owner']['login']\n",
    "            repo_stats['owner_type'] = j['owner']['type']\n",
    "            repo_stats['name'] = j['name']\n",
    "            repo_stats['description'] = j['description']\n",
    "            repo_stats['private'] = j['private']\n",
    "            repo_stats['fork'] = j['fork']\n",
    "            repo_stats['html_url'] = j['html_url']\n",
    "            repo_stats['language'] = j['language']\n",
    "            repo_stats['forks_count'] = j['forks_count']\n",
    "            repo_stats['stargazers_count'] = j['stargazers_count']\n",
    "            repo_stats['watchers_count'] = j['watchers_count']\n",
    "            repo_stats['subscribers_count'] = j['subscribers_count']\n",
    "            repo_stats['network_count'] = j['network_count']\n",
    "            repo_stats['size'] = j['size']\n",
    "            repo_stats['open_issues_count'] = j['open_issues_count']\n",
    "            repo_stats['has_issues'] = j['has_issues']\n",
    "            repo_stats['has_wiki'] = j['has_wiki']\n",
    "            repo_stats['has_pages'] = j['has_pages']\n",
    "            repo_stats['has_downloads'] = j['has_downloads']\n",
    "            repo_stats['pushed_at'] = j['pushed_at']\n",
    "            repo_stats['created_at'] = j['created_at']\n",
    "            repo_stats['updated_at'] = j['updated_at']\n",
    "            \n",
    "            all_repos.append(repo_stats)\n",
    "            \n",
    "        except:\n",
    "            msg = \"Repo %s metadata did not process\" % n\n",
    "            write_to_log(msg)\n",
    "                            \n",
    "\n",
    "print(\"\")            \n",
    "print(\"We have %s notebooks\"% len(all_repos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "9e093de96825"
   },
   "source": [
    "Okay, so it looks like we could not process 590 repos (193616 - 193026). Let's get a list of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "comet_cell_id": "03093a3101ee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_metadata_ids = []\n",
    "\n",
    "with open('../logs/repo_metadata_cleaning_log.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        missing_metadata_ids.append(int(l.split(' ')[1]))\n",
    "        \n",
    "len(missing_metadata_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "comet_cell_id": "90f494e26f732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": \"Not Found\", \"documentation_url\": \"https://developer.github.com/v3\"}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/api_results/repo_metadata/repo_%s.json' % missing_metadata_ids[400], 'r') as f:\n",
    "    for l in f:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "6cdfd67158258"
   },
   "source": [
    "Yep, it looks like we are just missing these. Let me spot check by looking for the one of the repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "5a135f2085928"
   },
   "outputs": [],
   "source": [
    "missing_repo_example = df_nbs[df_nbs.repo_id == missing_metadata_ids[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "comet_cell_id": "90fd41e8caa0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007851    https://github.com/gdmarmerola/hyper-optim-thesis\n",
       "1007852    https://github.com/gdmarmerola/hyper-optim-thesis\n",
       "Name: repo_html_url, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_repo_example.repo_html_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "2275c7f8d3965"
   },
   "source": [
    "Yep, this link is broken. Let's see how many notebooks in total we are missing if we exclude these 590 repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "comet_cell_id": "8f11ecb0403ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4161, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing_repo_nbs = df_nbs[df_nbs.repo_id.isin(missing_metadata_ids)]\n",
    "df_missing_repo_nbs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "3705dce9586c5"
   },
   "source": [
    "Bummer, that is a fair number of notebooks, though not as bad as I feared it could be. We will just not analyze these files as it seems the repositories were renamed, moved, or deleted in between our initial notebook query, and this follow-up query to get repository metadata.\n",
    "\n",
    "Now I should do the same pipeline for the readme files to see what data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "c01cf339f3c05"
   },
   "source": [
    "## Metadata Content Completeness\n",
    "Let's see what content we can load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "comet_cell_id": "8457a33794869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'path', 'sha', 'size', 'url', 'html_url', 'git_url', 'download_url', 'type', 'content', 'encoding', '_links'])\n"
     ]
    }
   ],
   "source": [
    "with open('../data/readmes/readme_%s.json' % repo_readme_ids[2]) as f:\n",
    "    test_file = json.load(f)\n",
    "    print(test_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "d6bee7bcc579a"
   },
   "source": [
    "From this list I think we want:\n",
    "1. name\n",
    "2. path\n",
    "3. size\n",
    "4. html_url\n",
    "5. type\n",
    "6. content - base64 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "comet_cell_id": "673a5d79190e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 193616 repo readme files processed\n",
      "10000 / 193616 repo readme files processed\n",
      "20000 / 193616 repo readme files processed\n",
      "30000 / 193616 repo readme files processed\n",
      "40000 / 193616 repo readme files processed\n",
      "50000 / 193616 repo readme files processed\n",
      "60000 / 193616 repo readme files processed\n",
      "70000 / 193616 repo readme files processed\n",
      "80000 / 193616 repo readme files processed\n",
      "90000 / 193616 repo readme files processed\n",
      "100000 / 193616 repo readme files processed\n",
      "110000 / 193616 repo readme files processed\n",
      "120000 / 193616 repo readme files processed\n",
      "130000 / 193616 repo readme files processed\n",
      "140000 / 193616 repo readme files processed\n",
      "150000 / 193616 repo readme files processed\n",
      "160000 / 193616 repo readme files processed\n",
      "170000 / 193616 repo readme files processed\n",
      "180000 / 193616 repo readme files processed\n",
      "190000 / 193616 repo readme files processed\n",
      "\n",
      "We have 142449 notebook readmes\n"
     ]
    }
   ],
   "source": [
    "all_readmes = []\n",
    "\n",
    "def write_to_log(msg):\n",
    "    f = '../logs/repo_readme_cleaning_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "for i, n in enumerate(repo_readme_ids):\n",
    "    \n",
    "    # keep track of our progress\n",
    "    if i % 10000 == 0:\n",
    "        print(\"%s / %s repo readme files processed\" % (i, len(repo_readme_ids)))\n",
    "    \n",
    "    with open('../data/readmes/readme_%s.json' % n, 'r') as json_file:\n",
    "\n",
    "        try:\n",
    "            j = json.load(json_file)\n",
    "            \n",
    "            readme_stats = {}\n",
    "            readme_stats['repo_id'] = n\n",
    "            readme_stats['name'] = j['name']\n",
    "            readme_stats['path'] = j['path']\n",
    "            readme_stats['size'] = j['size']\n",
    "            readme_stats['html_url'] = j['html_url']\n",
    "            readme_stats['type'] = j['type']\n",
    "            readme_stats['content'] = j['content']\n",
    "            \n",
    "            all_readmes.append(readme_stats)\n",
    "            \n",
    "        except:\n",
    "            msg = \"Repo %s readme did not process\" % n\n",
    "            write_to_log(msg)\n",
    "                            \n",
    "\n",
    "print(\"\")            \n",
    "print(\"We have %s notebook readmes\"% len(all_readmes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "comet_cell_id": "853ec012d754f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102334"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_readme_ids = []\n",
    "\n",
    "with open('../logs/repo_readme_cleaning_log.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        missing_readme_ids.append(int(l.split(' ')[1]))\n",
    "        \n",
    "len(missing_readme_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "14b9871b59d1e"
   },
   "source": [
    "Hmm, this is a much larger number of missing readmes. Let's take a closer look at these 51167 missing readmes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "comet_cell_id": "d7d5608e2aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": \"Not Found\", \"documentation_url\": \"https://developer.github.com/v3\"}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/readmes/readme_%s.json' % missing_readme_ids[3000], 'r') as f:\n",
    "    for l in f:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "c595021093ceb"
   },
   "source": [
    "So It looks like we've got quite a few missing readmes. Let's check to see if this is due to the readme not being there, or because the repo does not actually have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "comet_cell_id": "6ad602f1392"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169641     https://github.com/daveorstu/SFU-bootcamp-backup\n",
       "411614     https://github.com/daveorstu/SFU-bootcamp-backup\n",
       "1110447    https://github.com/daveorstu/SFU-bootcamp-backup\n",
       "1229011    https://github.com/daveorstu/SFU-bootcamp-backup\n",
       "Name: repo_html_url, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_readme_example = df_nbs[df_nbs.repo_id == missing_readme_ids[1000]]\n",
    "missing_readme_example.repo_html_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "e06f0f0664b74"
   },
   "source": [
    "Just spot checking a few examples, it looks like many of these are because the repo did not actually have a readme. That is encouraging, but we won't be able to tell how many of these missing readmes are due to the repo not having a readme, or because the repo was taken down inbetween when we downloaded the repo data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5b9905b371f22"
   },
   "source": [
    "## Download CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "07a1dfbada3bd"
   },
   "outputs": [],
   "source": [
    "df_repo_metadata = pd.DataFrame(all_repos)\n",
    "df_readmes = pd.DataFrame(all_readmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "9d3cf659a970c"
   },
   "outputs": [],
   "source": [
    "df_repo_metadata.to_csv('../data/csv/repo_metadata.csv')\n",
    "df_readmes.to_csv('../data/csv/repo_readme.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "6b45025719336"
   },
   "source": [
    "And that's a wrap. We now have the core data about our nbs and their repos in CSV files. We can now build on these by [calculating more stats about each notebook](6_compute_nb_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "b2a9005172468"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "comet_paths": [
   [
    "d1dd24ab/5_repo_metadata_cleaning.ipynb",
    1501098625679
   ]
  ],
  "comet_tracking": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
