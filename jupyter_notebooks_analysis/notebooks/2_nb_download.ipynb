{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "4f8aa33743e49"
   },
   "source": [
    "# Downloading Notebooks\n",
    "\n",
    "This notebook is devoted to downloading the actual notebook files from Github. This search began ~1.30p PST on Fri July 14, 2017 and finished 6.40p on Wednesday July 19, 2017.\n",
    "\n",
    "This downloading was done in batches to check data quality along the way and avoid getting blocked by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "6d07f26880358"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "5d2eb087f5973"
   },
   "source": [
    "First let's create our dataframe. Then we can write our scraping code and iteratively go through the file sizes by feeding the code different dataframes. I elected not to download all the files at once so I could check quality of the results along the way and hopefully avoid getting this IP address blocked by github. We are not using the Github API to do this download, so we won't get a 403: denied request message if we are pulling too much data. They may just shut down the IP instead which would be a hard stop on the download (and the project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "3abe1260bdbea"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_nb_data.csv')\n",
    "df.rename(columns = {'Unnamed: 0':'nb_id'}, inplace = True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "29f7b11852a2"
   },
   "outputs": [],
   "source": [
    "def write_to_log(msg):\n",
    "    f = 'nb_log.txt'\n",
    "    log_file = open(f, \"a\")\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "def scrape_nb_from_df(data_frame):\n",
    "    \n",
    "    # check the files already downloaded in case we need to restart the search\n",
    "    current_files = os.listdir('notebooks_under_100kb')\n",
    "    print(len(current_files))\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i, row in data_frame.iterrows():\n",
    "        \n",
    "        date_string = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        count += 1\n",
    "        \n",
    "        # keep track of the download progress, and don't download any files we already have\n",
    "        if count % 10000 == 0:\n",
    "            print(count)\n",
    "                \n",
    "        if 'nb_%s.ipynb' % row['nb_id'] in current_files:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # access the raw content webpage and download the file\n",
    "            raw_url = row['html_url'].replace('github.com','raw.githubusercontent.com')\n",
    "            raw_url = raw_url.replace('/blob', '')\n",
    "            r = requests.get(raw_url)\n",
    "\n",
    "            filename = 'notebooks_under_100kb/nb_%s.ipynb' % row['nb_id']\n",
    "            with open(filename, 'w') as nb_file:\n",
    "                nb_file.write(r.text)\n",
    "            \n",
    "            msg = \"%s: downloaded %s\" % (date_string, row['nb_id'])\n",
    "            write_to_log(msg)\n",
    "            \n",
    "            # if needed we can uncomment this line to slow down the downloads\n",
    "            # time.sleep(0.1)\n",
    "            \n",
    "        except:\n",
    "            msg = \"%s: had trouble downloading %s\" % (date_string, row['nb_id'])\n",
    "            write_to_log(msg)\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comet_cell_id": "8460b194e1cdc"
   },
   "source": [
    "## Download the files\n",
    "\n",
    "The download proceeded in three major batches, separated into different folders in case any one folder had too many files or too much data in it for OSX's finder to handle.\n",
    "\n",
    "1. Files over 1Mb search for in batches of over 100Mb, over 50Mb, over 30Mb and over 1Mb. In total this was 93,962 files and 390 GB of data\n",
    "2. Files between 100Kb and 1 Mb. Tis was 341,705 files and 118 Gb of data\n",
    "3. Finally files under 100Kb in size. This was 817,953 files and only 21 Gb of data.\n",
    "\n",
    "In total we have 1,253,620 files and 529 Gb of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "a912c0b7b1253"
   },
   "outputs": [],
   "source": [
    "df_over_100mb = df[df['max_filesize'] >= 100000000]\n",
    "print(df_over_100mb['max_filesize'].sum() / 1000000000)\n",
    "print(df_over_100mb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "46a1bd1e0b01f"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_over_100mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "b36b367199bf8"
   },
   "outputs": [],
   "source": [
    "df_over_50mb = df[(df['max_filesize'] >= 50000000) & (df['max_filesize'] < 100000000)]\n",
    "print(df_over_50mb['max_filesize'].sum() / 1000000000)\n",
    "print(df_over_50mb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "11aeb0cc5657c"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_over_50mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "1e5bf4b34025"
   },
   "outputs": [],
   "source": [
    "df_over_30mb = df[(df['max_filesize'] >= 30000000) & (df['max_filesize'] < 50000000)]\n",
    "print(df_over_30mb['max_filesize'].sum() / 1000000000)\n",
    "print(df_over_30mb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "aa056c8e89aae"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_over_30mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "73cf5b5e925a5"
   },
   "outputs": [],
   "source": [
    "df_over_1mb = df[(df['max_filesize'] >= 1000000) & (df['max_filesize'] < 30000000)]\n",
    "print(df_over_1mb['max_filesize'].sum() / 1000000000)\n",
    "print(df_over_1mb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "9c7dd84c0c133"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_over_1mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "076c58b2fc844"
   },
   "outputs": [],
   "source": [
    "df_over_100kb = df[(df['max_filesize'] >= 100000) & (df['max_filesize'] < 1000000)]\n",
    "print(df_over_100kb['max_filesize'].sum() / 1000000000)\n",
    "print(df_over_100kb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "463dc61c9378f"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_over_100kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "dac7cfc45a7e1"
   },
   "outputs": [],
   "source": [
    "df_under_100kb = df[df['max_filesize'] < 100000]\n",
    "print(df_under_100kb['max_filesize'].sum() / 1000000000)\n",
    "print(df_under_100kb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "fd8887947afdf"
   },
   "outputs": [],
   "source": [
    "scrape_nb_from_df(df_under_100kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "bf897164159fb"
   },
   "source": [
    "And that's a wrap. We have now downloaded all the nb files. All of our data should be downloaded at this point. We may want to go collect data on the commits for each notebook in the future, but this will take a long time. Even if there were just three commits per notebook, we would have to run one query to list these commits, and another query to get the details of each commit (for us the most relevant number is the number of lines added or removed). For 1.25 million notebooks, this is 5 million queries. With our limit of only 5,000 queries per hour on Github's API, that leaves with with 1000 hours or about 40 days of straight quering. I don't think its worth it at this time.\n",
    "\n",
    "Also, for cleanliness, we manually merged all the notebooks into a single folder under `data/notebooks` for future analyses.\n",
    "\n",
    "Now onto [cleaning the notebook](3_nb_cleaning.ipynb) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "comet_cell_id": "9cd1e365a4c31"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "comet_paths": [
   [
    "5e1a4093/downloading_nbs.ipynb",
    1500060337663
   ],
   [
    "b47baa7f/downloading_nbs.ipynb",
    1500565252837
   ],
   [
    "b47baa7f/3_downloading_nbs.ipynb",
    1500565281833
   ],
   [
    "d1dd24ab/3_downloading_nbs.ipynb",
    1501096112947
   ],
   [
    "d1dd24ab/2_nb_download.ipynb",
    1501096831270
   ]
  ],
  "comet_tracking": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
